{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256])\n",
      "Input shape: torch.Size([1, 3, 256, 256])\n",
      "Output shape: torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm that supports both channels_last and channels_first data formats\"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format='channels_last'):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        \n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.data_format == 'channels_last':\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == 'channels_first':\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x-u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x-u) / torch.sqrt(s+self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "            return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"ConvNeXT Block: depthwise conv, LayerNorm, pointwise conv\"\"\"\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)  # depthwise conv\n",
    "        self.norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim)  # pointwise/1x1 convs\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones(dim), requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "        \n",
    "        x = input + self.drop_path(x)\n",
    "        return x\n",
    "\n",
    "class ConvNeXt(nn.Module):\n",
    "    \"\"\"ConvNeXT backbone for UNet\"\"\"\n",
    "    def __init__(self, in_chans=3, depths=[3, 3, 9, 3], \n",
    "                 dims=[96, 192, 384, 768], drop_path_rate=0.,\n",
    "                 layer_scale_init_value=1e-6, out_indices=[0, 1, 2, 3]):\n",
    "        super().__init__()\n",
    "        self.out_indices = out_indices\n",
    "        \n",
    "        # Stem and 3 intermediate downsampling conv layers\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
    "        )\n",
    "        \n",
    "        self.downsample_layers = nn.ModuleList([stem])\n",
    "        \n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "        \n",
    "        # 4 feature resolution stages, each consisting of multiple residual blocks\n",
    "        self.stages = nn.ModuleList()\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n",
    "                       layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward_features(self, x):\n",
    "        features = []\n",
    "        \n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "            if i in self.out_indices:\n",
    "                features.append(x)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.forward_features(x)\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"Upsampling block for UNet decoder\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, skip_channels=0):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels + skip_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, skip=None):\n",
    "        x = self.up(x)\n",
    "        if skip is not None:\n",
    "            # Handle potential size mismatch\n",
    "            diffY = skip.size()[2] - x.size()[2]\n",
    "            diffX = skip.size()[3] - x.size()[3]\n",
    "            \n",
    "            x = F.pad(x, [diffX // 2, diffX - diffX // 2,\n",
    "                          diffY // 2, diffY - diffY // 2])\n",
    "            \n",
    "            x = torch.cat([skip, x], dim=1)\n",
    "        \n",
    "        return self.conv(x)\n",
    "\n",
    "class ConvNeXtUNet(nn.Module):\n",
    "    \"\"\"UNet with ConvNeXT backbone for image segmentation\"\"\"\n",
    "    def __init__(self, in_channels=3, num_classes=1, \n",
    "                 backbone_depths=[3, 3, 9, 3],\n",
    "                 backbone_dims=[96, 192, 384, 768],\n",
    "                 drop_path_rate=0.1,\n",
    "                 layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ConvNeXT backbone\n",
    "        self.backbone = ConvNeXt(\n",
    "            in_chans=in_channels,\n",
    "            depths=backbone_depths,\n",
    "            dims=backbone_dims,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            layer_scale_init_value=layer_scale_init_value\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.up1 = UpBlock(backbone_dims[3], backbone_dims[2], backbone_dims[2])\n",
    "        self.up2 = UpBlock(backbone_dims[2], backbone_dims[1], backbone_dims[1])\n",
    "        self.up3 = UpBlock(backbone_dims[1], backbone_dims[0], backbone_dims[0])\n",
    "        \n",
    "        # Final upsampling and output\n",
    "        self.up_final = nn.ConvTranspose2d(backbone_dims[0], backbone_dims[0] // 2, kernel_size=2, stride=2)\n",
    "        self.final_conv = nn.Conv2d(backbone_dims[0] // 2, backbone_dims[0] // 4, kernel_size=3, padding=1)\n",
    "        self.final_up = nn.ConvTranspose2d(backbone_dims[0] // 4, backbone_dims[0] // 8, kernel_size=2, stride=2)\n",
    "        self.final = nn.Conv2d(backbone_dims[0] // 8, num_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get features from backbone\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Decoder path\n",
    "        x = self.up1(features[3], features[2])\n",
    "        x = self.up2(x, features[1])\n",
    "        x = self.up3(x, features[0])\n",
    "        \n",
    "        # Final layers\n",
    "        x = self.up_final(x)\n",
    "        x = F.relu(self.final_conv(x))\n",
    "        x = self.final_up(x)\n",
    "        x = self.final(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example of how to create a ConvNeXtUNet with different configurations\n",
    "def convnext_unet_tiny(in_channels=3, num_classes=1):\n",
    "    return ConvNeXtUNet(\n",
    "        in_channels=in_channels,\n",
    "        num_classes=num_classes,\n",
    "        backbone_depths=[3, 3, 9, 3],\n",
    "        backbone_dims=[96, 192, 384, 768]\n",
    "    )\n",
    "\n",
    "def convnext_unet_small(in_channels=3, num_classes=1):\n",
    "    return ConvNeXtUNet(\n",
    "        in_channels=in_channels,\n",
    "        num_classes=num_classes,\n",
    "        backbone_depths=[3, 3, 27, 3],\n",
    "        backbone_dims=[96, 192, 384, 768]\n",
    "    )\n",
    "\n",
    "def convnext_unet_base(in_channels=3, num_classes=1):\n",
    "    return ConvNeXtUNet(\n",
    "        in_channels=in_channels,\n",
    "        num_classes=num_classes,\n",
    "        backbone_depths=[3, 3, 27, 3],\n",
    "        backbone_dims=[128, 256, 512, 1024]\n",
    "    )\n",
    "\n",
    "def convnext_unet_large(in_channels=3, num_classes=1):\n",
    "    return ConvNeXtUNet(\n",
    "        in_channels=in_channels,\n",
    "        num_classes=num_classes,\n",
    "        backbone_depths=[3, 3, 27, 3],\n",
    "        backbone_dims=[192, 384, 768, 1536]\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "model = convnext_unet_large(num_classes=1)\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "output = model(x)\n",
    "print(output.shape)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

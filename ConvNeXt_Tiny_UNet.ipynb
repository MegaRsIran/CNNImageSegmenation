{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models.convnext import convnext_tiny\n",
    "\n",
    "class ConvNeXt_UNet(nn.Module):\n",
    "    def __init__(self, num_classes=1, backbone_type='tiny'):\n",
    "        super(ConvNeXt_UNet, self).__init__()\n",
    "        \n",
    "        if backbone_type == 'tiny':\n",
    "            convnext = convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
    "            self.feature_channels = [96, 192, 384, 768]\n",
    "        elif backbone_type == 'small':\n",
    "            convnext = models.convnext_small(weights=models.ConvNeXt_Small_Weights.IMAGENET1K_V1)\n",
    "            self.feature_channels = [96, 192, 384, 768]\n",
    "        elif backbone_type == 'base':\n",
    "            convnext = models.convnext_base(weights=models.ConvNeXt_Base_Weights.IMAGENET1K_V1)\n",
    "\n",
    "            self.feature_channels = [128, 256, 512, 1024]\n",
    "        elif backbone_type == 'large':\n",
    "            convnext = models.convnext_large(weights=models.ConvNeXt_Large_Weights.IMAGENET1K_V1)\n",
    "            self.feature_channels = [192, 384, 768, 1536]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone type: {backbone_type}\")\n",
    "        \n",
    "        self.stem = convnext.features[0]                   # Initial conv + norm\n",
    "        self.downsample1 = convnext.features[1]            # Downsample to 1/4\n",
    "        self.stage1 = convnext.features[2]                 # Stage 1\n",
    "        self.downsample2 = convnext.features[3]            # Downsample to 1/8\n",
    "        self.stage2 = convnext.features[4]                 # Stage 2\n",
    "        self.downsample3 = convnext.features[5]            # Downsample to 1/16\n",
    "        self.stage3 = convnext.features[6]                 # Stage 3\n",
    "        self.downsample4 = convnext.features[7]            # Downsample to 1/32\n",
    "        self.stage4 = convnext.features[8]                 # Stage 4\n",
    "        \n",
    "        # Decoder path - adapted for ConvNeXt dimensions\n",
    "        self.upconv1 = self.upconv(self.feature_channels[3], self.feature_channels[2])\n",
    "        self.upconv2 = self.upconv(self.feature_channels[2] * 2, self.feature_channels[1])\n",
    "        self.upconv3 = self.upconv(self.feature_channels[1] * 2, self.feature_channels[0])\n",
    "        self.upconv4 = self.upconv(self.feature_channels[0] * 2, self.feature_channels[0] // 2)\n",
    "        \n",
    "        # Final layers\n",
    "        self.final_upsample = nn.ConvTranspose2d(self.feature_channels[0] // 2, 32, kernel_size=2, stride=2)\n",
    "        self.final_conv = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "    \n",
    "    def upconv(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder path with ConvNeXt\n",
    "        x0 = self.stem(x)                  # 1/4 resolution\n",
    "        \n",
    "        x1 = self.downsample1(x0)          # Already at 1/4 from stem, this is identity\n",
    "        x1 = self.stage1(x1)               # Stage 1 features\n",
    "        \n",
    "        x2 = self.downsample2(x1)          # 1/8 resolution\n",
    "        x2 = self.stage2(x2)               # Stage 2 features\n",
    "        \n",
    "        x3 = self.downsample3(x2)          # 1/16 resolution\n",
    "        x3 = self.stage3(x3)               # Stage 3 features\n",
    "        \n",
    "        x4 = self.downsample4(x3)          # 1/32 resolution\n",
    "        x4 = self.stage4(x4)               # Stage 4 features (bottleneck)\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        d1 = self.upconv1(x4)              # Upsampled from bottleneck\n",
    "        d1 = torch.cat([d1, x3], dim=1)    # Skip connection with stage 3\n",
    "        \n",
    "        d2 = self.upconv2(d1)              # Upsampled\n",
    "        d2 = torch.cat([d2, x2], dim=1)    # Skip connection with stage 2\n",
    "        \n",
    "        d3 = self.upconv3(d2)              # Upsampled\n",
    "        d3 = torch.cat([d3, x1], dim=1)    # Skip connection with stage 1\n",
    "        \n",
    "        d4 = self.upconv4(d3)              # Upsampled\n",
    "        \n",
    "        d5 = self.final_upsample(d4)       # Final upsampling to original resolution\n",
    "        out = self.final_conv(d5)          # Final 1x1 convolution\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Test the ConvNeXt Tiny UNet model\n",
    "convnext_tiny_unet = ConvNeXt_UNet(num_classes=1, backbone_type='tiny')\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "output = convnext_tiny_unet(x)\n",
    "print(\"ConvNeXt-Tiny UNet output shape:\", output.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
